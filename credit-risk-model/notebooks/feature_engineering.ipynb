{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c081e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Assuming your training data is in the 'data' directory\n",
    "data_path = '../data/raw/training.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# IMPORTANT: Ensure the TransactionStartTime column is a datetime object\n",
    "df['TransactionStartTime'] = pd.to_datetime(df['TransactionStartTime'])\n",
    "\n",
    "# If you had missing values in your previous EDA, this is where you'd handle them,\n",
    "# but since the EDA confirmed NO MISSING VALUES, we can skip that.\n",
    "\n",
    "# Now, the rest of your original code will execute without the NameError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e061a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# ... (other imports)\n",
    "\n",
    "# ===============================================================\n",
    "# FIX: LOAD DATA AND CONVERT DATETIME\n",
    "# ===============================================================\n",
    "data_path = '../data/raw/training.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df['TransactionStartTime'] = pd.to_datetime(df['TransactionStartTime'])\n",
    "# ===============================================================\n",
    "\n",
    "\n",
    "# --- 1. Define Reference Date and Calculate RFM ---\n",
    "# Find the maximum transaction date in the dataset\n",
    "max_transaction_date = df['TransactionStartTime'].max() # This line will now work!\n",
    "\n",
    "# Define the reference date as 1 day after the last transaction\n",
    "REFERENCE_DATE = max_transaction_date + timedelta(days=1)\n",
    "\n",
    "# Group by CustomerId and aggregate to calculate RFM\n",
    "# ... (rest of your RFM code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "522ae192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RFM Aggregated Data Sample ---\n",
      "        CustomerId  Recency  Frequency  Monetary    ChannelId    ProviderId\n",
      "0     CustomerId_1       84          1  -10000.0  ChannelId_2  ProviderId_4\n",
      "1    CustomerId_10       84          1  -10000.0  ChannelId_2  ProviderId_4\n",
      "2  CustomerId_1001       90          5   20000.0  ChannelId_3  ProviderId_4\n",
      "3  CustomerId_1002       26         11    4225.0  ChannelId_2  ProviderId_4\n",
      "4  CustomerId_1003       12          6   20000.0  ChannelId_3  ProviderId_6\n"
     ]
    }
   ],
   "source": [
    "# Group by CustomerId and aggregate to calculate RFM\n",
    "df_rfm = df.groupby('CustomerId').agg(\n",
    "    # Recency: Calculate the difference between the reference date and max transaction date\n",
    "    Recency=('TransactionStartTime', lambda x: (REFERENCE_DATE - x.max()).days),\n",
    "    \n",
    "    # Frequency: Count the number of transactions\n",
    "    Frequency=('TransactionId', 'count'),\n",
    "    \n",
    "    # Monetary: Sum of the chosen monetary feature (using 'Amount' and dropping 'Value' later)\n",
    "    Monetary=('Amount', 'sum'),\n",
    "    \n",
    "    # Add key categorical features for later encoding (use mode for customer-level representation)\n",
    "    ChannelId=('ChannelId', lambda x: x.mode()[0] if not x.mode().empty else 'Unknown'),\n",
    "    ProviderId=('ProviderId', lambda x: x.mode()[0] if not x.mode().empty else 'Unknown')\n",
    ").reset_index()\n",
    "\n",
    "# Display the head of the aggregated RFM data\n",
    "print(\"--- RFM Aggregated Data Sample ---\")\n",
    "print(df_rfm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d963014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapperTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer that caps outliers above a specified percentile (e.g., 99th).\n",
    "    Fits the threshold on the training data and uses it for transforming.\n",
    "    \"\"\"\n",
    "    def __init__(self, upper_percentile=99):\n",
    "        self.upper_percentile = upper_percentile\n",
    "        self.thresholds = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_df = pd.DataFrame(X)\n",
    "        for col in X_df.columns:\n",
    "            self.thresholds[col] = X_df[col].quantile(self.upper_percentile / 100)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = pd.DataFrame(X, copy=True)\n",
    "        for col, threshold in self.thresholds.items():\n",
    "            # Apply capping\n",
    "            X_copy[col] = np.clip(X_copy[col], a_min=None, a_max=threshold)\n",
    "        return X_copy.values # Return NumPy array for pipeline compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56ef8265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Processed Feature Matrix (Sample) ---\n",
      "   Monetary   Recency  Frequency  ChannelId_ChannelId_1  \\\n",
      "0       NaN  1.937605  -0.253459                    0.0   \n",
      "1       NaN  1.937605  -0.253459                    0.0   \n",
      "2 -0.039434  2.158882  -0.212186                    0.0   \n",
      "3 -0.817819 -0.201408  -0.150278                    0.0   \n",
      "4 -0.039434 -0.717722  -0.201868                    0.0   \n",
      "\n",
      "   ChannelId_ChannelId_2  ChannelId_ChannelId_3  ChannelId_ChannelId_5  \\\n",
      "0                    1.0                    0.0                    0.0   \n",
      "1                    1.0                    0.0                    0.0   \n",
      "2                    0.0                    1.0                    0.0   \n",
      "3                    1.0                    0.0                    0.0   \n",
      "4                    0.0                    1.0                    0.0   \n",
      "\n",
      "   ProviderId_ProviderId_1  ProviderId_ProviderId_2  ProviderId_ProviderId_3  \\\n",
      "0                      0.0                      0.0                      0.0   \n",
      "1                      0.0                      0.0                      0.0   \n",
      "2                      0.0                      0.0                      0.0   \n",
      "3                      0.0                      0.0                      0.0   \n",
      "4                      0.0                      0.0                      0.0   \n",
      "\n",
      "   ProviderId_ProviderId_4  ProviderId_ProviderId_5  ProviderId_ProviderId_6  \n",
      "0                      1.0                      0.0                      0.0  \n",
      "1                      1.0                      0.0                      0.0  \n",
      "2                      1.0                      0.0                      0.0  \n",
      "3                      1.0                      0.0                      0.0  \n",
      "4                      0.0                      0.0                      1.0  \n",
      "\n",
      "Shape of Final Matrix: (3742, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Her\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:390: RuntimeWarning: invalid value encountered in log1p\n",
      "  return func(X, **(kw_args if kw_args else {}))\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Define Feature Sets ---\n",
    "MONETARY_FEATS = ['Monetary']\n",
    "RF_FEATS = ['Recency', 'Frequency']\n",
    "CATEGORICAL_FEATS = ['ChannelId', 'ProviderId']\n",
    "\n",
    "# --- 4. Define Specialized Pipelines ---\n",
    "# 4a. Monetary Pipeline (Capping, Log Transform, Scaling)\n",
    "monetary_pipeline = Pipeline(steps=[\n",
    "    # 1. Custom Capper for Outliers (EDA Finding)\n",
    "    ('capper', CapperTransformer(upper_percentile=99)), \n",
    "    \n",
    "    # 2. Log Transformation for Skewness (EDA Finding)\n",
    "    # Using numpy.log1p for log(1 + x)\n",
    "    ('log_transform', FunctionTransformer(np.log1p, validate=True)),\n",
    "    \n",
    "    # 3. Standardization\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 4b. Recency and Frequency Pipeline (Scaling Only)\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 4c. Categorical Pipeline (One-Hot Encoding)\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# --- 5. Create the Column Transformer ---\n",
    "# This applies different pipelines to different feature subsets\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('monetary_proc', monetary_pipeline, MONETARY_FEATS),\n",
    "        ('rf_proc', rf_pipeline, RF_FEATS),\n",
    "        ('cat_proc', categorical_pipeline, CATEGORICAL_FEATS)\n",
    "    ],\n",
    "    # Drop the customer ID and any other unlisted columns\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "# --- 6. Define the Final Feature Engineering Pipeline ---\n",
    "feat_engineering_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "# --- 7. Fit and Transform the RFM Data ---\n",
    "# Note: Only fit on the training data.\n",
    "X_processed_array = feat_engineering_pipeline.fit_transform(df_rfm)\n",
    "\n",
    "# Convert the resulting array back to a DataFrame for easier handling\n",
    "# The column names will be complex due to OneHotEncoding prefixing\n",
    "feature_names = (\n",
    "    MONETARY_FEATS + RF_FEATS + \n",
    "    list(feat_engineering_pipeline.named_steps['preprocessor'].transformers_[2][1]['onehot'].get_feature_names_out(CATEGORICAL_FEATS))\n",
    ")\n",
    "\n",
    "df_processed = pd.DataFrame(X_processed_array, columns=feature_names)\n",
    "\n",
    "print(\"\\n--- Final Processed Feature Matrix (Sample) ---\")\n",
    "print(df_processed.head())\n",
    "print(\"\\nShape of Final Matrix:\", df_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6029a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: model_artifacts\n",
      "Pipeline successfully saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the directory\n",
    "ARTIFACTS_DIR = 'model_artifacts'\n",
    "\n",
    "# Check if the directory exists, and if not, create it\n",
    "if not os.path.exists(ARTIFACTS_DIR):\n",
    "    os.makedirs(ARTIFACTS_DIR)\n",
    "    print(f\"Created directory: {ARTIFACTS_DIR}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {ARTIFACTS_DIR}\")\n",
    "\n",
    "# --- Now run your dump command ---\n",
    "import joblib\n",
    "joblib.dump(feat_engineering_pipeline, os.path.join(ARTIFACTS_DIR, 'feat_engineering_pipeline.pkl'))\n",
    "\n",
    "print(\"Pipeline successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f356319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
