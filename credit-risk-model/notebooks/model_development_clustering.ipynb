{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a12784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# You must also redefine the custom class if you haven't yet!\n",
    "class CapperTransformer(BaseEstimator, TransformerMixin):\n",
    "    # ... (paste the full CapperTransformer class definition here)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19b797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import pandas as pd # Needed for the Capper to use quantiles\n",
    "\n",
    "# =================================================================\n",
    "# FIX: REDEFINE CUSTOM TRANSFORMER FOR JOBLIB TO FIND IT\n",
    "# =================================================================\n",
    "class CapperTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer that caps outliers above a specified percentile (e.g., 99th).\n",
    "    Must be defined here so joblib.load() can successfully reconstruct the pipeline.\n",
    "    \"\"\"\n",
    "    def __init__(self, upper_percentile=99):\n",
    "        self.upper_percentile = upper_percentile\n",
    "        self.thresholds = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_df = pd.DataFrame(X)\n",
    "        for col in X_df.columns:\n",
    "            self.thresholds[col] = X_df[col].quantile(self.upper_percentile / 100)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = pd.DataFrame(X, copy=True)\n",
    "        for col, threshold in self.thresholds.items():\n",
    "            # Apply capping\n",
    "            X_copy[col] = np.clip(X_copy[col], a_min=None, a_max=threshold)\n",
    "        return X_copy.values # Return NumPy array for pipeline compatibility\n",
    "# ================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b6575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# FIX: RE-CREATE df_rfm (Required Input for Clustering)\n",
    "# You need the original raw transaction data (df) to perform this step.\n",
    "# =================================================================\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Assuming the raw data path is correct from previous steps:\n",
    "data_path = '../data/raw/training.csv' \n",
    "df = pd.read_csv(data_path)\n",
    "df['TransactionStartTime'] = pd.to_datetime(df['TransactionStartTime'])\n",
    "\n",
    "# Define Reference Date (Must be consistent with Task 3)\n",
    "REFERENCE_DATE = df['TransactionStartTime'].max() + timedelta(days=1)\n",
    "\n",
    "# Re-calculate RFM (Re-running Task 3, Part 1)\n",
    "df_rfm = df.groupby('CustomerId').agg(\n",
    "    Recency=('TransactionStartTime', lambda x: (REFERENCE_DATE - x.max()).days),\n",
    "    Frequency=('TransactionId', 'count'),\n",
    "    Monetary=('Amount', 'sum'),\n",
    "    ChannelId=('ChannelId', lambda x: x.mode()[0] if not x.mode().empty else 'Unknown'),\n",
    "    ProviderId=('ProviderId', lambda x: x.mode()[0] if not x.mode().empty else 'Unknown')\n",
    ").reset_index()\n",
    "\n",
    "# Now df_rfm is defined and ready for clustering.\n",
    "# ================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6725b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering Pipeline loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from datetime import timedelta\n",
    "\n",
    "# --- STEP 1: DEFINE CUSTOM CLASS (CRUCIAL for joblib to load the pipeline) ---\n",
    "class CapperTransformer(BaseEstimator, TransformerMixin):\n",
    "    # ... (Your CapperTransformer definition must be here)\n",
    "    def __init__(self, upper_percentile=99):\n",
    "        self.upper_percentile = upper_percentile\n",
    "        self.thresholds = {}\n",
    "    def fit(self, X, y=None):\n",
    "        X_df = pd.DataFrame(X)\n",
    "        X_df.columns = [f'col_{i}' for i in range(X_df.shape[1])] # Dummy columns for safety\n",
    "        for col in X_df.columns:\n",
    "            self.thresholds[col] = X_df[col].quantile(self.upper_percentile / 100)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_copy = pd.DataFrame(X, copy=True)\n",
    "        X_copy.columns = list(self.thresholds.keys())\n",
    "        for col, threshold in self.thresholds.items():\n",
    "            X_copy[col] = np.clip(X_copy[col], a_min=None, a_max=threshold)\n",
    "        return X_copy.values\n",
    "\n",
    "\n",
    "# --- STEP 2: LOAD DATA (Define df_rfm) ---\n",
    "# Re-run Task 3 Part 1 aggregation to define df_rfm\n",
    "data_path = '../data/raw/training.csv' \n",
    "df = pd.read_csv(data_path)\n",
    "df['TransactionStartTime'] = pd.to_datetime(df['TransactionStartTime'])\n",
    "REFERENCE_DATE = df['TransactionStartTime'].max() + timedelta(days=1)\n",
    "df_rfm = df.groupby('CustomerId').agg(\n",
    "    Recency=('TransactionStartTime', lambda x: (REFERENCE_DATE - x.max()).days),\n",
    "    Frequency=('TransactionId', 'count'),\n",
    "    Monetary=('Amount', 'sum'),\n",
    "    ChannelId=('ChannelId', lambda x: x.mode()[0] if not x.mode().empty else 'Unknown'),\n",
    "    ProviderId=('ProviderId', lambda x: x.mode()[0] if not x.mode().empty else 'Unknown')\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "# --- STEP 3: LOAD PIPELINE (Define feat_engineering_pipeline) ---\n",
    "try:\n",
    "    feat_engineering_pipeline = joblib.load('model_artifacts/feat_engineering_pipeline.pkl')\n",
    "    print(\"Feature Engineering Pipeline loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"CRITICAL ERROR: feat_engineering_pipeline.pkl not found. Cannot proceed.\")\n",
    "    \n",
    "# --- STEP 4: DEFINE FEATURES ---\n",
    "RFM_FEATS = ['Recency', 'Frequency', 'Monetary'] \n",
    "CATEGORICAL_FEATS = ['ChannelId', 'ProviderId'] \n",
    "CLUSTER_FEATURES = RFM_FEATS + CATEGORICAL_FEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95a69b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled feature matrix shape: (3742, 13)\n",
      "\n",
      "Total NaN count in X_scaled before clustering: 192\n",
      "❌ ERROR: The pipeline fix did not work! NaNs still present.\n",
      "\n",
      "Sample rows in X_scaled containing NaNs:\n",
      "    0         1         2    3    4    5    6    7    8    9    10   11   12\n",
      "0  NaN  1.937605 -0.253459  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
      "1  NaN  1.937605 -0.253459  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
      "27 NaN  2.195761 -0.232823  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
      "69 NaN  1.937605 -0.253459  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
      "78 NaN  1.384412 -0.232823  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Her\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:390: RuntimeWarning: invalid value encountered in log1p\n",
      "  return func(X, **(kw_args if kw_args else {}))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming feat_engineering_pipeline is loaded and df_rfm is defined in memory.\n",
    "# Re-define features just to be safe:\n",
    "RFM_FEATS = ['Recency', 'Frequency', 'Monetary'] \n",
    "CATEGORICAL_FEATS = ['ChannelId', 'ProviderId'] \n",
    "CLUSTER_FEATURES = RFM_FEATS + CATEGORICAL_FEATS\n",
    "\n",
    "# --- 1. Define X_raw ---\n",
    "# Assumes df_rfm (the RFM aggregated data) is loaded/re-created in a previous cell.\n",
    "X_raw = df_rfm[CLUSTER_FEATURES]\n",
    "\n",
    "# --- 2. Apply the Loaded Pipeline to Create X_scaled ---\n",
    "# THIS is the line that defines X_scaled.\n",
    "X_scaled = feat_engineering_pipeline.transform(X_raw)\n",
    "print(f\"Scaled feature matrix shape: {X_scaled.shape}\")\n",
    "\n",
    "# --- 3. DIAGNOSTIC CHECK ---\n",
    "X_scaled_df = pd.DataFrame(X_scaled) \n",
    "\n",
    "# Check for any NaNs in the scaled data\n",
    "nan_count = X_scaled_df.isna().sum().sum()\n",
    "print(f\"\\nTotal NaN count in X_scaled before clustering: {nan_count}\")\n",
    "\n",
    "# Final confirmation printout\n",
    "if nan_count > 0:\n",
    "    print(\"❌ ERROR: The pipeline fix did not work! NaNs still present.\")\n",
    "    nan_rows = X_scaled_df[X_scaled_df.isna().any(axis=1)]\n",
    "    print(\"\\nSample rows in X_scaled containing NaNs:\")\n",
    "    print(nan_rows.head())\n",
    "else:\n",
    "    print(\"✅ SUCCESS: NaN check passed! X_scaled is clean and ready for clustering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd4dece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed Pipeline defined in memory.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Assuming CapperTransformer is defined in the environment.\n",
    "\n",
    "# 1. Monetary Pipeline (Recency, Frequency, Monetary)\n",
    "# The order is crucial: Capper -> Log1p (creates NaNs) -> IMPUTER (cleans NaNs) -> Scaler\n",
    "monetary_pipeline = Pipeline(steps=[\n",
    "    ('capper', CapperTransformer(upper_percentile=99)), \n",
    "    ('log_transform', FunctionTransformer(np.log1p, validate=True)),\n",
    "    # ADDED IMPUTATION HERE: This is what you need to fix the NaNs from log1p\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 2. Recency and Frequency Pipeline (They have the same pre-processing)\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('capper', CapperTransformer(upper_percentile=99)), \n",
    "    ('log_transform', FunctionTransformer(np.log1p, validate=True)),\n",
    "    # ADDED IMPUTATION HERE\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 3. Categorical Pipeline\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# 4. Column Transformer\n",
    "RFM_FEATS = ['Recency', 'Frequency', 'Monetary'] \n",
    "CATEGORICAL_FEATS = ['ChannelId', 'ProviderId'] \n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('monetary', monetary_pipeline, ['Monetary']),\n",
    "        ('rf', rf_pipeline, ['Recency', 'Frequency']),\n",
    "        ('cat', categorical_pipeline, CATEGORICAL_FEATS)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# 5. Final Pipeline (This is the object you need to save!)\n",
    "feat_engineering_pipeline_FIXED = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # K-Means will be added later for Task 4 completion, but for now we only fit the preprocessor\n",
    "])\n",
    "print(\"Fixed Pipeline defined in memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eaf75a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed Pipeline successfully fitted to data.\n",
      "Fixed Pipeline artifact saved. The old one has been overwritten.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Her\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:390: RuntimeWarning: invalid value encountered in log1p\n",
      "  return func(X, **(kw_args if kw_args else {}))\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Assuming df_rfm is in memory and CLUSTER_FEATURES are defined.\n",
    "\n",
    "# Define the features needed for fitting the ColumnTransformer\n",
    "RFM_FEATS = ['Recency', 'Frequency', 'Monetary'] \n",
    "CATEGORICAL_FEATS = ['ChannelId', 'ProviderId'] \n",
    "CLUSTER_FEATURES = RFM_FEATS + CATEGORICAL_FEATS\n",
    "\n",
    "# Fit the FIXED pipeline to your data\n",
    "# Note: We use feat_engineering_pipeline_FIXED, which was defined in the previous cell.\n",
    "X_fit = df_rfm[CLUSTER_FEATURES]\n",
    "feat_engineering_pipeline_FIXED.fit(X_fit)\n",
    "print(\"Fixed Pipeline successfully fitted to data.\")\n",
    "\n",
    "# CRITICAL STEP: Save the fixed, fitted pipeline\n",
    "# We are saving it with the original filename to overwrite the old, flawed artifact.\n",
    "joblib.dump(feat_engineering_pipeline_FIXED, 'model_artifacts/feat_engineering_pipeline.pkl')\n",
    "print(\"Fixed Pipeline artifact saved. The old one has been overwritten.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fbbe669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New (Clean) Pipeline loaded successfully.\n",
      "\n",
      "Final NaN count in X_scaled: 0\n",
      "✅ SUCCESS: Data is clean and ready for clustering!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Her\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:390: RuntimeWarning: invalid value encountered in log1p\n",
      "  return func(X, **(kw_args if kw_args else {}))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering complete. Assigned 4 clusters to 3742 customers.\n",
      "Ready to analyze clusters and assign the 'is_high_risk' target.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_rfm and CLUSTER_FEATURES are defined in memory.\n",
    "\n",
    "# 1. Reload the new, clean pipeline\n",
    "# We must reload it to ensure the artifact on disk (the clean one) is what we use.\n",
    "feat_engineering_pipeline = joblib.load('model_artifacts/feat_engineering_pipeline.pkl')\n",
    "print(\"New (Clean) Pipeline loaded successfully.\")\n",
    "\n",
    "# 2. Re-run transformation and diagnostic check\n",
    "X_raw = df_rfm[CLUSTER_FEATURES]\n",
    "X_scaled = feat_engineering_pipeline.transform(X_raw)\n",
    "X_scaled_df = pd.DataFrame(X_scaled) \n",
    "nan_count = X_scaled_df.isna().sum().sum()\n",
    "\n",
    "print(f\"\\nFinal NaN count in X_scaled: {nan_count}\")\n",
    "\n",
    "if nan_count == 0:\n",
    "    print(\"✅ SUCCESS: Data is clean and ready for clustering!\")\n",
    "    \n",
    "    # --- PROCEED TO CLUSTERING ---\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    K = 4  # Assuming K=4 was chosen for your analysis\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "    kmeans = KMeans(n_clusters=K, random_state=RANDOM_STATE, n_init=10)\n",
    "\n",
    "    # Fit the model to the scaled data and assign cluster IDs\n",
    "    df_rfm['Cluster_ID'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    print(f\"\\nClustering complete. Assigned {K} clusters to {len(df_rfm)} customers.\")\n",
    "    print(\"Ready to analyze clusters and assign the 'is_high_risk' target.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ CRITICAL FAILURE: NaNs still present. Something is wrong with the Imputer setup or order.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e561a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cluster Profiles (Mean RFM Values) ---\n",
      "            Customer_Count  Recency_Days  Frequency_Mean  Monetary_Mean\n",
      "Cluster_ID                                                             \n",
      "1                     1095             6            66.9      518447.73\n",
      "2                     1382            38            11.2      115757.39\n",
      "0                      201            40            24.6     -575898.36\n",
      "3                     1064            48             1.8       28873.33\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd # Import pandas again for safety\n",
    "\n",
    "# --- 1. Calculate Cluster Profiles ---\n",
    "# Group the original df_rfm by the new Cluster_ID\n",
    "cluster_profiles = df_rfm.groupby('Cluster_ID')[['Recency', 'Frequency', 'Monetary']].mean()\n",
    "\n",
    "# Add the count of customers in each cluster\n",
    "cluster_profiles['Customer_Count'] = df_rfm['Cluster_ID'].value_counts()\n",
    "\n",
    "# Calculate the descriptive statistics for easier reading\n",
    "cluster_profiles['Recency_Days'] = cluster_profiles['Recency'].round(0).astype(int)\n",
    "cluster_profiles['Frequency_Mean'] = cluster_profiles['Frequency'].round(1)\n",
    "cluster_profiles['Monetary_Mean'] = cluster_profiles['Monetary'].round(2)\n",
    "\n",
    "# Select final columns for display\n",
    "cluster_profiles = cluster_profiles[['Customer_Count', 'Recency_Days', 'Frequency_Mean', 'Monetary_Mean']]\n",
    "\n",
    "print(\"--- Cluster Profiles (Mean RFM Values) ---\")\n",
    "print(cluster_profiles.sort_values(by='Recency_Days', ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b5a9bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Target Variable Distribution ---\n",
      "is_high_risk\n",
      "0    71.6\n",
      "1    28.4\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Final customer dataset with target saved to: data/processed/df_customer_target.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Target Assignment ---\n",
    "# Based on analysis, Cluster 3 has the highest Recency (48 days) and lowest Frequency (1.8),\n",
    "# making it the highest churn risk proxy.\n",
    "HIGH_RISK_CLUSTER_ID = 3 \n",
    "\n",
    "# Create the final target variable\n",
    "df_rfm['is_high_risk'] = np.where(df_rfm['Cluster_ID'] == HIGH_RISK_CLUSTER_ID, 1, 0)\n",
    "\n",
    "# Check the distribution of the target variable\n",
    "target_distribution = df_rfm['is_high_risk'].value_counts(normalize=True).mul(100).round(1)\n",
    "\n",
    "print(\"\\n--- Target Variable Distribution ---\")\n",
    "print(target_distribution)\n",
    "\n",
    "# --- 2. Final Save ---\n",
    "OUTPUT_FILE_PATH = 'data/processed/df_customer_target.csv'\n",
    "# Ensure you are saving the dataframe df_rfm which now contains 'Cluster_ID' and 'is_high_risk'\n",
    "df_rfm.to_csv(OUTPUT_FILE_PATH, index=False)\n",
    "print(f\"\\nFinal customer dataset with target saved to: {OUTPUT_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4208d135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xverse\n",
      "  Using cached xverse-1.0.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xverse) (2.3.5)\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xverse) (1.7.2)\n",
      "Requirement already satisfied: scipy>=0.19.0 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xverse) (1.16.3)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xverse) (0.14.5)\n",
      "Requirement already satisfied: pandas>=0.21.1 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xverse) (2.3.3)\n",
      "Requirement already satisfied: matplotlib>=3.0.3 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xverse) (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.3->xverse) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.3->xverse) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.3->xverse) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.3->xverse) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.3->xverse) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.3->xverse) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.3->xverse) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.3->xverse) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=0.21.1->xverse) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=0.21.1->xverse) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.3->xverse) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=0.19.0->xverse) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=0.19.0->xverse) (3.6.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\her\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsmodels>=0.6.1->xverse) (1.0.2)\n",
      "Using cached xverse-1.0.5-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: xverse\n",
      "Successfully installed xverse-1.0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package 'xverse' has an invalid Requires-Python: Invalid specifier: '>=3.5.*'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# This command tells the notebook to run a shell command to install the package\n",
    "# into the Python environment that the notebook is currently using.\n",
    "!{sys.executable} -m pip install xverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db9e7d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with target. Shape: (3742, 8)\n",
      "Train/Test Split complete. Train shape: (2993, 5)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "WOE.__init__() got an unexpected keyword argument 'Continuous_Cols'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain/Test Split complete. Train shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# --- 4. Initialize and Fit WOE Transformer (EXPLICIT FIX) ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m woe_transformer = \u001b[43mWOE\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Specify features explicitly to guide the transformer\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mContinuous_Cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONTINUOUS_FEATS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mDiscrete_Cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDISCRETE_FEATS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# CRITICAL FIX: Explicitly disable monotonic binning, which is causing the AttributeError\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmonotonic_binning\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m)\u001b[49m \n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Fit the transformer on the TRAINING data only\u001b[39;00m\n\u001b[32m     41\u001b[39m woe_transformer.fit(X_train, y_train)\n",
      "\u001b[31mTypeError\u001b[39m: WOE.__init__() got an unexpected keyword argument 'Continuous_Cols'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xverse.transformer import WOE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# --- 1. Load Data with Target Variable ---\n",
    "df_final = pd.read_csv('data/processed/df_customer_target.csv')\n",
    "print(f\"Loaded dataset with target. Shape: {df_final.shape}\")\n",
    "\n",
    "# --- 2. Define Features and Target ---\n",
    "# Ensure FEATURE_COLS matches the features you need (RFM, Channel, Provider)\n",
    "FEATURE_COLS = ['Recency', 'Frequency', 'Monetary', 'ChannelId', 'ProviderId'] \n",
    "TARGET_COL = 'is_high_risk'\n",
    "\n",
    "# Explicitly define feature types for the WOE transformer\n",
    "CONTINUOUS_FEATS = ['Recency', 'Frequency', 'Monetary']\n",
    "DISCRETE_FEATS = ['ChannelId', 'ProviderId']\n",
    "\n",
    "X = df_final[FEATURE_COLS]\n",
    "y = df_final[TARGET_COL]\n",
    "\n",
    "# --- 3. Train/Test Split (80/20) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Train/Test Split complete. Train shape: {X_train.shape}\")\n",
    "\n",
    "# --- 4. Initialize and Fit WOE Transformer (EXPLICIT FIX) ---\n",
    "\n",
    "woe_transformer = WOE(\n",
    "    # Specify features explicitly to guide the transformer\n",
    "    Continuous_Cols=CONTINUOUS_FEATS,\n",
    "    Discrete_Cols=DISCRETE_FEATS,\n",
    "    # CRITICAL FIX: Explicitly disable monotonic binning, which is causing the AttributeError\n",
    "    monotonic_binning=False \n",
    ") \n",
    "\n",
    "# Fit the transformer on the TRAINING data only\n",
    "woe_transformer.fit(X_train, y_train)\n",
    "\n",
    "# Inspect Information Value (IV) Analysis\n",
    "iv_df = woe_transformer.iv_df.sort_values(by='IV', ascending=False)\n",
    "print(\"\\n--- Information Value (IV) Analysis ---\")\n",
    "print(iv_df)\n",
    "\n",
    "# Apply WOE Transformation to both sets\n",
    "X_train_woe = woe_transformer.transform(X_train)\n",
    "X_test_woe = woe_transformer.transform(X_test)\n",
    "print(\"\\nWoE Transformation Complete.\")\n",
    "\n",
    "# --- 5. Model Training (Logistic Regression) ---\n",
    "model = LogisticRegression(random_state=42, solver='liblinear')\n",
    "model.fit(X_train_woe, y_train)\n",
    "print(\"Logistic Regression Model Trained.\")\n",
    "\n",
    "# --- 6. Model Evaluation ---\n",
    "y_pred_proba = model.predict_proba(X_test_woe)[:, 1]\n",
    "auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "y_pred = model.predict(X_test_woe)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n--- Model Evaluation ---\")\n",
    "print(f\"AUC-ROC Score: {auc_roc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# --- 7. Save Model and WOE Transformer ---\n",
    "joblib.dump(model, 'model_artifacts/logistic_regression_model.pkl')\n",
    "joblib.dump(woe_transformer, 'model_artifacts/woe_transformer.pkl')\n",
    "print(\"\\nModel and WOE Transformer saved to artifacts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0086c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WOE Constructor Arguments in Your Environment ---\n",
      "(self, feature_names='all', exclude_features=None, woe_prefix=None, treat_missing='separate', woe_bins=None, monotonic_binning=True, mono_feature_names='all', mono_max_bins=20, mono_force_bins=3, mono_cardinality_cutoff=5, mono_prefix=None, mono_custom_binning=None)\n"
     ]
    }
   ],
   "source": [
    "from xverse.transformer import WOE\n",
    "import inspect\n",
    "\n",
    "# Print the accepted arguments for the WOE constructor in your environment\n",
    "print(\"--- WOE Constructor Arguments in Your Environment ---\")\n",
    "print(inspect.signature(WOE.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71f73364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install optbinning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bea054de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with target. Shape: (3742, 8)\n",
      "Train/Test Split complete. Train shape: (2993, 5)\n",
      "\n",
      "Fitting OptimalBinning (WoE Transformation)...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'OptimalBinning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFitting OptimalBinning (WoE Transformation)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m RFM_FEATS:\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Set the binning properties for each continuous RFM feature\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     optb = \u001b[43mOptimalBinning\u001b[49m(name=feature, dtype=\u001b[33m\"\u001b[39m\u001b[33mnumerical\u001b[39m\u001b[33m\"\u001b[39m, solver=\u001b[33m\"\u001b[39m\u001b[33mcp\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# Fit the transformer and get the binning table\u001b[39;00m\n\u001b[32m     40\u001b[39m     optb.fit(X_train[feature], y_train)\n",
      "\u001b[31mNameError\u001b[39m: name 'OptimalBinning' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load Data with Target Variable ---\n",
    "df_final = pd.read_csv('data/processed/df_customer_target.csv')\n",
    "print(f\"Loaded dataset with target. Shape: {df_final.shape}\")\n",
    "\n",
    "# --- 2. Define Features and Target ---\n",
    "RFM_FEATS = ['Recency', 'Frequency', 'Monetary']\n",
    "CATEGORICAL_FEATS = ['ChannelId', 'ProviderId']\n",
    "FEATURE_COLS = RFM_FEATS + CATEGORICAL_FEATS\n",
    "TARGET_COL = 'is_high_risk'\n",
    "\n",
    "X = df_final[FEATURE_COLS]\n",
    "y = df_final[TARGET_COL]\n",
    "\n",
    "# --- 3. Train/Test Split (80/20) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Train/Test Split complete. Train shape: {X_train.shape}\")\n",
    "\n",
    "# --- 4. Feature Transformation (Using optbinning) ---\n",
    "woe_mapping = {}\n",
    "iv_results = []\n",
    "\n",
    "# A. Loop through RFM features and fit OptimalBinning\n",
    "print(\"\\nFitting OptimalBinning (WoE Transformation)...\")\n",
    "for feature in RFM_FEATS:\n",
    "    # Set the binning properties for each continuous RFM feature\n",
    "    optb = OptimalBinning(name=feature, dtype=\"numerical\", solver=\"cp\")\n",
    "    \n",
    "    # Fit the transformer and get the binning table\n",
    "    optb.fit(X_train[feature], y_train)\n",
    "    \n",
    "    # Store the transformation details\n",
    "    woe_mapping[feature] = optb\n",
    "    \n",
    "    # Extract Information Value\n",
    "    iv_table = optb.binning_table.build()\n",
    "    iv_value = iv_table['IV'].sum()\n",
    "    iv_results.append({'Variable_Name': feature, 'IV': iv_value})\n",
    "\n",
    "# B. Extract Information Value (IV) Analysis\n",
    "iv_df = pd.DataFrame(iv_results).sort_values(by='IV', ascending=False)\n",
    "print(\"\\n--- Information Value (IV) Analysis (optbinning) ---\")\n",
    "print(iv_df)\n",
    "\n",
    "# C. Apply WoE Transformation (using the stored mapping)\n",
    "def apply_woe(df, mapping):\n",
    "    df_woe = pd.DataFrame(index=df.index)\n",
    "    for feature, optb in mapping.items():\n",
    "        # Get the WoE values directly from the fitted transformer\n",
    "        df_woe[f'{feature}_woe'] = optb.transform(df[feature], metric=\"woe\")\n",
    "    return df_woe\n",
    "\n",
    "X_train_woe_rfm = apply_woe(X_train, woe_mapping)\n",
    "X_test_woe_rfm = apply_woe(X_test, woe_mapping)\n",
    "print(\"\\nWoE Transformation Complete using optbinning.\")\n",
    "\n",
    "\n",
    "# D. Initialize and Fit OneHotEncoder (for Categorical features - unchanged)\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "ohe.fit(X_train[CATEGORICAL_FEATS])\n",
    "\n",
    "# Apply OHE Transformation\n",
    "X_train_ohe_cat = ohe.transform(X_train[CATEGORICAL_FEATS])\n",
    "X_test_ohe_cat = ohe.transform(X_test[CATEGORICAL_FEATS])\n",
    "\n",
    "# Convert OHE output to DataFrame\n",
    "cat_col_names = ohe.get_feature_names_out(CATEGORICAL_FEATS)\n",
    "X_train_ohe_cat_df = pd.DataFrame(X_train_ohe_cat, columns=cat_col_names, index=X_train.index)\n",
    "X_test_ohe_cat_df = pd.DataFrame(X_test_ohe_cat, columns=cat_col_names, index=X_test.index)\n",
    "\n",
    "# E. Concatenate the transformed features\n",
    "X_train_final = pd.concat([X_train_woe_rfm.reset_index(drop=True), X_train_ohe_cat_df.reset_index(drop=True)], axis=1)\n",
    "X_test_final = pd.concat([X_test_woe_rfm.reset_index(drop=True), X_test_ohe_cat_df.reset_index(drop=True)], axis=1)\n",
    "print(\"Feature Transformation Complete and Combined.\")\n",
    "\n",
    "# --- 5. Model Training (Logistic Regression) ---\n",
    "model = LogisticRegression(random_state=42, solver='liblinear')\n",
    "model.fit(X_train_final, y_train)\n",
    "\n",
    "print(\"\\nLogistic Regression Model Trained.\")\n",
    "\n",
    "# --- 6. Model Evaluation ---\n",
    "y_pred_proba = model.predict_proba(X_test_final)[:, 1]\n",
    "auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "y_pred = model.predict(X_test_final)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n--- Model Evaluation ---\")\n",
    "print(f\"AUC-ROC Score: {auc_roc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# --- 7. Save Model and Transformers ---\n",
    "# Save only the necessary artifacts\n",
    "joblib.dump(model, 'model_artifacts/logistic_regression_model.pkl')\n",
    "joblib.dump(woe_mapping, 'model_artifacts/woe_mapping_optbinning.pkl') # Save the new mapping\n",
    "joblib.dump(ohe, 'model_artifacts/ohe_transformer.pkl') \n",
    "print(\"\\nModel, optbinning WoE Mapping, and OHE saved to artifacts.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
